{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3fe621f-ef21-4081-9175-2ede965a3bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuoraConstant:\n",
    "    link = 'beir/quora/test'\n",
    "    name = 'quora'\n",
    "    cosine_threshold = 0.3\n",
    "    ngram = (1, 2)\n",
    "    min_df = 2\n",
    "    max_df = 0.8\n",
    "    vectorized_path = 'quora_vectorizer.pkl'\n",
    "    cluster_path = 'quora_cluster.pkl'\n",
    "    tfidf_path = ''\n",
    "\n",
    "\n",
    "class ClinicalConstant:\n",
    "    link = 'clinicaltrials/2017/trec-pm-2017'\n",
    "    name = 'clinical'\n",
    "    cosine_threshold = 0.001\n",
    "    ngram = (1, 3)\n",
    "    min_df = 2\n",
    "    max_df = 0.8\n",
    "    vectorized_path = 'clinical_vectorizer.pkl'\n",
    "    cluster_path = 'clinical_cluster.pkl'\n",
    "    tfidf_path = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d665f96-9a14-4e71-bc0b-9e2bd650fb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import re\n",
    "from nltk import WordNetLemmatizer, pos_tag\n",
    "import num2words\n",
    "import numpy as np\n",
    "import pycountry\n",
    "from nltk import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def replace_under_score_with_space(text: str) -> str:\n",
    "    new_tokens = []\n",
    "    for token in text.split():\n",
    "        new_tokens.append(re.sub(r'_', ' ', token))\n",
    "    return \" \".join(new_tokens)\n",
    "\n",
    "\n",
    "def remove_stopwords(text: str, dataset: str) -> str:\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    questions = {'what', 'which', 'who', 'where', 'why', 'when', 'how', 'whose', 'how often', 'how long', 'how far',\n",
    "                 'how old', 'how come', 'how much', 'how many', 'what type', 'what kind', 'which type', 'which kind'}\n",
    "    if dataset == 'quora':\n",
    "        stop_words = stop_words - questions\n",
    "    else:\n",
    "        stop_words = stop_words - {'a'}\n",
    "    return \" \".join([word for word in str(text).split() if word not in stop_words])\n",
    "\n",
    "def remove_punctuations(text: str) -> str:\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    result = tokenizer.tokenize(text)\n",
    "    return \" \".join(result)\n",
    "\n",
    "\n",
    "def remove_markers(text: str) -> str:\n",
    "    normalized_tokens = []\n",
    "    for token in text.split():\n",
    "        normalized_tokens.append(re.sub(r'\\u00AE', '', token))\n",
    "    return \" \".join(normalized_tokens)\n",
    "\n",
    "def _normalize_country_names(text: str) -> str:\n",
    "    normalized_tokens = []\n",
    "    for token in text.split():\n",
    "        upper_token = token.upper()\n",
    "        country_name = None\n",
    "\n",
    "        # Try to lookup by alpha-2 code\n",
    "        country = pycountry.countries.get(alpha_2=upper_token)\n",
    "        if not country:\n",
    "            # Try to lookup by alpha-3 code if alpha-2 lookup fails\n",
    "            country = pycountry.countries.get(alpha_3=upper_token)\n",
    "\n",
    "        if country:\n",
    "            country_name = country.name\n",
    "\n",
    "        # Append the found country name or the original token if not found\n",
    "        normalized_tokens.append(country_name if country_name else token)\n",
    "\n",
    "    return \" \".join(normalized_tokens)\n",
    "\n",
    "\n",
    "\n",
    "def convert_numbers(text: str) -> str:\n",
    "    new_text = []\n",
    "    for w in text.split():\n",
    "        try:\n",
    "            w = num2words(int(w))\n",
    "        except:\n",
    "            a = 0\n",
    "        new_text.append(w)\n",
    "    new_text = np.char.replace(new_text, \"-\", \" \")\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "\n",
    "def remove_commas_from_numbers(text: str) -> str:\n",
    "    # Define the regex pattern\n",
    "    pattern = r'(?<=\\d),(?=\\d)'\n",
    "    new_text = []\n",
    "    # Process each string in the list\n",
    "    for w in text.split():\n",
    "        # Replace commas that are between two digits\n",
    "        w = re.sub(pattern, '', w)\n",
    "        new_text.append(w)\n",
    "\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "\n",
    "\n",
    "def lowercase_letters(text: str) -> str:\n",
    "    return text.lower()\n",
    "\n",
    "def lemmatize_words(text: str) -> str:\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tagged_tokens = pos_tag(text.split())\n",
    "    return \" \".join([lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in tagged_tokens])\n",
    "\n",
    "\n",
    "# perform part-of-speech (POS) tagging on the tokens.\n",
    "def get_wordnet_pos(tag: str) -> str:\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def _normalize_dates(text: str) -> str:\n",
    "    date_pattern = r'(\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4})|' \\\n",
    "                   r'(\\d{4}[-/]\\d{1,2}[-/]\\d{1,2})|' \\\n",
    "                   r'(\\d{1,2}\\s+(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s+\\d{2,4})|' \\\n",
    "                   r'((Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s+\\d{1,2},\\s+\\d{2,4})|' \\\n",
    "                   r'(\\d{1,2}\\s+(January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{2,4})|' \\\n",
    "                   r'((January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2},\\s+\\d{2,4})'\n",
    "    format_strings = ['%d-%m-%Y', '%d/%m/%Y', '%d.%m.%Y', '%Y-%m-%d', '%Y/%m/%d', '%Y.%m.%d', '%d %b %Y', '%b %d, %Y',\n",
    "                      '%d %B %Y', '%B %d, %Y', '%m/%d/%Y', '%m-%d-%Y', '%m.%d.%Y', '%d-%m-%y', '%d/%m/%y', '%d.%m.%y',\n",
    "                      '%y-%m-%d', '%y/%m/%d', '%y.%m.%d', '%d %b %y', '%b %d, %y',\n",
    "                      '%d %B %y', '%B %d, %y', '%m/%d/%y', '%m-%d-%y', '%m.%d.%y']\n",
    "    normalized_tokens = []\n",
    "    for token in text.split():\n",
    "        matches = re.findall(date_pattern, token)\n",
    "        if matches:\n",
    "            match = matches[0][0]\n",
    "            for fmt in format_strings:\n",
    "                try:\n",
    "                    date_obj = datetime.strptime(match, fmt)\n",
    "                    break\n",
    "                except ValueError:\n",
    "                    pass\n",
    "            else:\n",
    "                continue\n",
    "            normalized_date = date_obj.strftime('%Y-%m-%d')\n",
    "            token = token.replace(match, normalized_date)\n",
    "        normalized_tokens.append(token)\n",
    "\n",
    "    return \" \".join(normalized_tokens)\n",
    "\n",
    "\n",
    "def remove_apostrophe(text: str) -> str:\n",
    "    new_tokens = []\n",
    "    for token in text.split():\n",
    "        new_tokens.append(str(np.char.replace(token, \"'\", \" \")))\n",
    "    return \" \".join(new_tokens)\n",
    "\n",
    "\n",
    "def normalize_abbreviations(text: str) -> str:\n",
    "    resolved_terms = {}\n",
    "    new_tokens = []\n",
    "\n",
    "    for token in text.split():\n",
    "        synsets = wordnet.synsets(token)\n",
    "        if synsets:\n",
    "            resolved_term = synsets[0].lemmas()[0].name()\n",
    "            resolved_terms[token] = resolved_term\n",
    "            new_tokens.append(resolved_term)\n",
    "        else:\n",
    "            new_tokens.append(token)\n",
    "\n",
    "    return \" \".join(new_tokens)\n",
    "\n",
    "\n",
    "def process_text_quora(text: str) -> str:\n",
    "    lowercase = lowercase_letters(text)\n",
    "    normalize_numbers = remove_commas_from_numbers(lowercase)\n",
    "    num2word = convert_numbers(normalize_numbers)\n",
    "    punctuations_removed = remove_punctuations(num2word)\n",
    "    apostrophe_removed = remove_apostrophe(punctuations_removed)\n",
    "    stopwords_removed = remove_stopwords(apostrophe_removed, 'quora')\n",
    "    # markers_removed = remove_markers(stopwords_removed)\n",
    "    # stemmed = stem_words(markers_removed)\n",
    "    normalized_dates = _normalize_dates(stopwords_removed)\n",
    "    normalized_country_names = _normalize_country_names(normalized_dates)\n",
    "    abbreviations = normalize_abbreviations(normalized_country_names)\n",
    "    lowercase = lowercase_letters(abbreviations)\n",
    "    new_tokens = replace_under_score_with_space(lowercase)\n",
    "    lemmatized = lemmatize_words(new_tokens)\n",
    "    new_tokens = lemmatized\n",
    "    return new_tokens\n",
    "\n",
    "\n",
    "def process_text_clinical(text: str) -> str:\n",
    "    lowercase = lowercase_letters(text)\n",
    "    normalize_numbers = remove_commas_from_numbers(lowercase)\n",
    "    num2word = convert_numbers(normalize_numbers)\n",
    "    punctuations_removed = remove_punctuations(num2word)\n",
    "    apostrophe_removed = remove_apostrophe(punctuations_removed)\n",
    "    stopwords_removed = remove_stopwords(apostrophe_removed, 'clinical')\n",
    "    markers_removed = remove_markers(stopwords_removed)\n",
    "    # stemmed = stem_words(markers_removed)\n",
    "    normalized_dates = _normalize_dates(markers_removed)\n",
    "    normalized_country_names = _normalize_country_names(normalized_dates)\n",
    "    abbreviations = normalize_abbreviations(normalized_country_names)\n",
    "    lowercase = lowercase_letters(abbreviations)\n",
    "    new_tokens = replace_under_score_with_space(lowercase)\n",
    "    lemmatized = lemmatize_words(new_tokens)\n",
    "    new_tokens = lemmatized\n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27d3680e-f290-4109-a053-4cfc0ca62d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from pymongo import MongoClient\n",
    "import ir_datasets\n",
    "\n",
    "def get_dataset_docs(dataset_name: str) -> Dict[str, str]:\n",
    "    print(\"get dataset docs \" + dataset_name)\n",
    "    i = 0\n",
    "    if dataset_name == \"clinical\":\n",
    "        dataset = ir_datasets.load(\"clinicaltrials/2017/trec-pm-2017\")\n",
    "        docs_iter = dataset.docs_iter()\n",
    "        random_corpus = {}\n",
    "        for doc in docs_iter:\n",
    "            doc_id = doc[0]\n",
    "            detailed_description = (doc[1] + ' ' + doc[2] + ' ' + doc[3] + ' '\n",
    "                                    + doc[4] + ' ' + doc[5])\n",
    "            random_corpus[doc_id] = detailed_description\n",
    "        random_corpus_ids = set(random_corpus.keys())\n",
    "        search_qrels = list(ir_datasets.load(\"clinicaltrials/2017/trec-pm-2017\").qrels_iter())\n",
    "        qrels_docs_ids = set(qrel.doc_id for qrel in search_qrels)\n",
    "        docs_ids = random_corpus_ids.union(qrels_docs_ids)\n",
    "        docs_store = ir_datasets.load(\"clinicaltrials/2017/trec-pm-2017\").docs_store()\n",
    "        mapped_docs = dict(docs_store.get_many(docs_ids))\n",
    "        corpus = {\n",
    "            doc_id: doc.title + ' ' + doc.condition + ' ' + doc.summary +\n",
    "                    ' ' + doc.detailed_description + ' ' + doc.eligibility\n",
    "            for doc_id, doc in mapped_docs.items()\n",
    "        }\n",
    "    else:  # dataset_name == \"quora\":\n",
    "        random_corpus = dict(ir_datasets.load(\"beir/quora/test\").docs_iter())\n",
    "        random_corpus_ids = set(random_corpus.keys())\n",
    "        qrels = list(ir_datasets.load(\"beir/quora/test\").qrels_iter())\n",
    "        qrels_docs_ids = set(qrel.doc_id for qrel in qrels)\n",
    "        docs_ids = random_corpus_ids.union(qrels_docs_ids)\n",
    "        docs_store = ir_datasets.load(\"beir/quora/test\").docs_store()\n",
    "        mapped_docs = dict(docs_store.get_many(docs_ids))\n",
    "        corpus = {}\n",
    "        for doc_id, doc in mapped_docs.items():\n",
    "            # i += 1\n",
    "            # if i >= 10000:\n",
    "            #     break\n",
    "            doc_id = doc[0]\n",
    "            detailed_description = (doc[1])\n",
    "            corpus[doc_id] = detailed_description\n",
    "        # corpus = {doc_id: doc.text for doc_id, doc in mapped_docs.items()}\n",
    "\n",
    "    print(len(corpus))\n",
    "    mongo_client = MongoClient('localhost', 27017)\n",
    "    db = mongo_client['IR_DOCS']\n",
    "    collection = db[dataset_name]\n",
    "    collection.drop()\n",
    "    documents_to_insert = []\n",
    "    index = 0\n",
    "    for doc_id, text in corpus.items():\n",
    "        documents_to_insert.append({'index': index, \"_id\": doc_id, \"text\": text})\n",
    "        index += 1\n",
    "    collection.insert_many(documents_to_insert)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf3200d9-ac8b-415e-bf63-df648334a1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_tfidf_matrix(dataset_name: str, inverted_index: dict):\n",
    "    with open(dataset_name + '_inverted_index.pickle', 'wb') as file:\n",
    "        pickle.dump(inverted_index, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd04cebb-0080-4823-b8e9-14e6d0ed31c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import joblib\n",
    "from nltk import word_tokenize\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def create_inverted_index(dataset: str):\n",
    "    print(\"create inverted index \" + dataset)\n",
    "    corpus = get_dataset_docs(dataset)\n",
    "    print(\"after get_dataset_docs \" + dataset)\n",
    "    if dataset == QuoraConstant.name:\n",
    "        vectorizer = TfidfVectorizer(preprocessor=process_text_quora, tokenizer=word_tokenize,\n",
    "                                     # max_df=QuoraConstant.max_df,\n",
    "                                     # min_df=QuoraConstant.min_df,\n",
    "                                     # norm=\"l2\",\n",
    "                                     # ngram_range=QuoraConstant.ngram,\n",
    "                                     # sublinear_tf=True,\n",
    "                                     # use_idf=True,\n",
    "                                     # use_idf=True,\n",
    "\n",
    "                                     )\n",
    "\n",
    "    else:\n",
    "        vectorizer = TfidfVectorizer(preprocessor=process_text_clinical, tokenizer=word_tokenize,\n",
    "                                     # max_df=ClinicalConstant.max_df,\n",
    "                                     # min_df=ClinicalConstant.min_df,\n",
    "                                     norm=\"l2\",\n",
    "                                     # ngram_range=ClinicalConstant.ngram,\n",
    "                                     sublinear_tf=True,\n",
    "                                     use_idf=True,\n",
    "                                     stop_words='english',\n",
    "                                     lowercase=False,\n",
    "                                     )\n",
    "\n",
    "    print(\"before fit transform \" + dataset)\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus.values())\n",
    "    if dataset == QuoraConstant.name:\n",
    "        joblib.dump(vectorizer, QuoraConstant.vectorized_path)\n",
    "    else:\n",
    "        joblib.dump(vectorizer, ClinicalConstant.vectorized_path)\n",
    "    print(\"after fit transform \" + dataset)\n",
    "    save_tfidf_matrix(dataset, tfidf_matrix)\n",
    "    # save_doc_ids(dataset, corpus.keys())\n",
    "    return tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6c7d2fe-f491-4214-930f-f0a511c9f8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tfidf_matrix(dataset_name: str) -> dict:\n",
    "    with open(dataset_name + '_inverted_index.pickle', 'rb') as file:\n",
    "        tfidf_matrix = pickle.load(file)\n",
    "    return tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "117adef6-833f-476e-aa92-9960884698d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def customize_result_from_index_to_doc_id(result, dataset: str):\n",
    "    mongo_client = MongoClient('localhost', 27017)\n",
    "    db = mongo_client['IR_DOCS']\n",
    "    collection = db[dataset]\n",
    "    data = collection.find({\"index\": {\"$in\": [int(s) for s in list(result)]}}, {'_id': 1, 'index': 1})\n",
    "    new_result = {}\n",
    "    for item in data:\n",
    "        new_result[item['_id']] = result[str(item['index'])]\n",
    "    return new_result\n",
    "\n",
    "def retrieve(query, dataset, tfidf_matrix) -> Dict[str, float]:\n",
    "    result = ranking(query, dataset, tfidf_matrix)\n",
    "    return customize_result_from_index_to_doc_id(result, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4e34628-0039-4c75-835e-c44f876004c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import ir_measures\n",
    "import joblib\n",
    "from ir_measures import R, AP, P, RR\n",
    "\n",
    "def evaluate(dataset_collection, dataset_name):\n",
    "    qrels = dataset_collection.qrels_iter()\n",
    "    queries = dataset_collection.queries_iter()\n",
    "    ranking_docs = dict()\n",
    "    i = 0\n",
    "    tfidf_matrix = load_tfidf_matrix(dataset_name)\n",
    "    for query in queries:\n",
    "        if dataset_name == QuoraConstant.name:\n",
    "            retrieved_docs = retrieve(query.text, dataset_name, tfidf_matrix)\n",
    "        else:\n",
    "            if query.other == 'None':\n",
    "                text = query.disease + ' ' + query.gene + ' ' + query.demographic\n",
    "            else:\n",
    "                text = query.disease + ' ' + query.gene + ' ' + query.demographic + ' ' + query.other\n",
    "            print(text)\n",
    "            retrieved_docs = retrieve(text, dataset_name, tfidf_matrix)\n",
    "        ranking_docs[query.query_id] = retrieved_docs\n",
    "        i += 1\n",
    "        if i % 100 == 99:\n",
    "            break\n",
    "    metrics = [AP(rel=1), P(rel=1) @ 10, R(rel=1) @ 10, RR(rel=1)]\n",
    "    qrels_map = dict()\n",
    "    for qrel in qrels:\n",
    "        if qrel.query_id in ranking_docs.keys():\n",
    "            if qrel.query_id in qrels_map:\n",
    "                qrels_map[qrel.query_id].update({qrel.doc_id: qrel.relevance})\n",
    "            else:\n",
    "                qrels_map[qrel.query_id] = {qrel.doc_id: qrel.relevance}\n",
    "    score = ir_measures.calc_aggregate(metrics, qrels_map, ranking_docs)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96c577f0-f13b-48bd-a032-d8ba9555fd20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create inverted index clinical\n",
      "get dataset docs clinical\n",
      "241006\n",
      "after get_dataset_docs clinical\n",
      "before fit transform clinical\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "D:\\Program\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['advance', 'afterlife', 'angstrom', 'buttock', 'buzzword', 'cipher', 'colombia', 'congress', 'demur', 'discovery', 'egypt', 'erstwhile', 'erythematosus', 'factory', 'follow', 'frankincense', 'frequently', 'furthermore', 'germany', 'inside', 'interim', 'iraqi', 'ireland', 'largely', 'look', 'lupus', 'make', 'midst', 'moon', 'nation', 'national', 'one-third', 'option', 'person', 'peru', 'point', 'possibly', 'recently', 'return', 'state', 'stop', 'subsequently', 'sum', 'support', 'unite', 'united'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after fit transform clinical\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<241006x265248 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 39668590 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_inverted_index(\"clinical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18b5f61a-4f60-4011-98cf-a23473d0b845",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Dict\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class VectorizeQuery:\n",
    "    quora_vectorizer = joblib.load(QuoraConstant.vectorized_path)\n",
    "    clinica_vectorizer = joblib.load(ClinicalConstant.vectorized_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_vectorize(query, dataset):\n",
    "        if dataset == QuoraConstant.name:\n",
    "            query_vector = VectorizeQuery.quora_vectorizer\n",
    "        else:\n",
    "            query_vector = VectorizeQuery.clinica_vectorizer\n",
    "        return query_vector.transform([query])\n",
    "\n",
    "def ranking(query: str, dataset: str, tfidf_matrix) -> Dict[str, float]:\n",
    "    if dataset == QuoraConstant.name:\n",
    "        cosine_threshold = QuoraConstant.cosine_threshold\n",
    "    else:\n",
    "        cosine_threshold = ClinicalConstant.cosine_threshold\n",
    "    query_vector = VectorizeQuery.get_vectorize(query, dataset)\n",
    "    similarities = cosine_similarity(query_vector, tfidf_matrix)\n",
    "    matched_indices = similarities.argsort()[0][::-1].flatten()\n",
    "    top_indices = []\n",
    "    for i in matched_indices:\n",
    "        if similarities[0][i] >= cosine_threshold:\n",
    "            top_indices.append(i.item())\n",
    "    results = {}\n",
    "    for index in top_indices:\n",
    "        doc_id = str(index)\n",
    "        similarity = similarities[0][index]\n",
    "        results[doc_id] = similarity\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18175fb1-d26a-4de4-853b-37d11bc26f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation score for beir/quora/test: {AP: 0.6761926014667079, RR: 0.7591320983854364, R@10: 0.756593331354097, P@10: 0.12323232323232308}\n"
     ]
    }
   ],
   "source": [
    "dataset_quora = ir_datasets.load(\"beir/quora/test\")\n",
    "score_quora = evaluate(dataset_quora, \"quora\")\n",
    "print(\"Evaluation score for beir/quora/test:\", score_quora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "751b9f1a-5fca-4cfd-8ad0-59ea7c376ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liposarcoma CDK4 Amplification 38-year-old male GERD\n",
      "Colon cancer KRAS (G13D), BRAF (V600E) 52-year-old male Type II Diabetes, Hypertension\n",
      "Meningioma NF2 (K322), AKT1(E17K) 45-year-old female\n",
      "Breast cancer FGFR1 Amplification, PTEN (Q171) 67-year-old female Depression, Hypertension, Heart Disease\n",
      "Melanoma BRAF (V600E), CDKN2A Deletion 45-year-old female\n",
      "Melanoma NRAS (Q61K) 55-year-old male Hypertension\n",
      "Lung cancer EGFR (L858R) 50-year-old female Lupus\n",
      "Lung cancer EML4-ALK Fusion transcript 52-year-old male Hypertension, Osteoarthritis\n",
      "Gastrointestinal stromal tumor KIT Exon 9 (A502_Y503dup) 49-year-old female\n",
      "Lung adenocarcinoma KRAS (G12C) 61-year-old female Hypertension, Hypercholesterolemia\n",
      "Gastric cancer PIK3CA (E545K) 54-year-old male Depression\n",
      "Colon cancer BRAF (V600E) 35-year-old male\n",
      "Cholangiocarcinoma BRCA2 72-year-old male Diabetes\n",
      "Cholangiocarcinoma IDH1 (R132H) 64-year-old male Neuropathy\n",
      "Cervical cancer STK11 26-year-old female\n",
      "Pancreatic cancer CDKN2A 54-year-old male Diabetes, Hypertension\n",
      "Prostate cancer PTEN Inactivating 81-year-old male Hypertension, Depression\n",
      "Pancreatic cancer CDK6 Amplification 48-year-old male\n",
      "Colorectal cancer FGFR1 Amplification 35-year-old female\n",
      "Liposarcoma MDM2 Amplification 26-year-old male\n",
      "Lung adenocarcinoma ALK Fusion 64-year-old female Emphysema\n",
      "Lung cancer ERBB2 Amplification 70-year-old male Arthritis\n",
      "Breast cancer PTEN Loss 54-year-old female Congestive Heart Failure\n",
      "Lung cancer NTRK1 58-year-old female Depression, Hypertension, Diabetes\n",
      "Lung adenocarcinoma MET Amplification 48-year-old male Emphysema\n",
      "Breast cancer NRAS Amplification 35-year-old female\n",
      "Pancreatic adenocarcinoma KRAS, TP53 49-year-old female\n",
      "Pancreatic ductal adenocarcinoma ERBB3 73-year-old female Whipple, FNA\n",
      "Ampullary carcinoma KRAS 61-year-old male\n",
      "Pancreatic adenocarcinoma RB1, TP53, KRAS 57-year-old female\n",
      "Evaluation score for clinicaltrials/2017/trec-pm-2017: {AP: 0.15944560641644115, RR: 0.41338951472555086, R@10: 0.0959921441196334, P@10: 0.21724137931034473}\n"
     ]
    }
   ],
   "source": [
    "dataset_clinical = ir_datasets.load(\"clinicaltrials/2017/trec-pm-2017\")\n",
    "score_clinical = evaluate(dataset_clinical, \"clinical\")\n",
    "print(\"Evaluation score for clinicaltrials/2017/trec-pm-2017:\", score_clinical)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
