{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27ba5273-8dee-4466-879f-a1ca0fe1aaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuoraConstant:\n",
    "    link = 'beir/quora/test'\n",
    "    name = 'quora'\n",
    "    cosine_threshold = 0.3\n",
    "    ngram = (1, 2)\n",
    "    min_df = 2\n",
    "    max_df = 0.8\n",
    "    vectorized_path = 'quora_vectorizer.pkl'\n",
    "    cluster_path = 'quora_cluster.pkl'\n",
    "\n",
    "class ClinicalConstant:\n",
    "    link = 'clinicaltrials/2017/trec-pm-2017'\n",
    "    name = 'clinical'\n",
    "    cosine_threshold = 0.001\n",
    "    ngram = (1, 3)\n",
    "    min_df = 2\n",
    "    max_df = 0.8\n",
    "    vectorized_path = 'clinical_vectorizer.pkl'\n",
    "    cluster_path = 'clinical_cluster.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96790edd-73d5-4188-aa48-18d5f4936873",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import re\n",
    "from nltk import WordNetLemmatizer, pos_tag\n",
    "import num2words\n",
    "import numpy as np\n",
    "import pycountry\n",
    "from nltk import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def replace_under_score_with_space(text: str) -> str:\n",
    "    new_tokens = []\n",
    "    for token in text.split():\n",
    "        new_tokens.append(re.sub(r'_', ' ', token))\n",
    "    return \" \".join(new_tokens)\n",
    "\n",
    "\n",
    "def remove_stopwords(text: str, dataset: str) -> str:\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    questions = {'what', 'which', 'who', 'where', 'why', 'when', 'how', 'whose', 'how often', 'how long', 'how far',\n",
    "                 'how old', 'how come', 'how much', 'how many', 'what type', 'what kind', 'which type', 'which kind'}\n",
    "    if dataset == 'quora':\n",
    "        stop_words = stop_words - questions\n",
    "    else:\n",
    "        stop_words = stop_words - {'a'}\n",
    "    return \" \".join([word for word in str(text).split() if word not in stop_words])\n",
    "\n",
    "def remove_punctuations(text: str) -> str:\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    result = tokenizer.tokenize(text)\n",
    "    return \" \".join(result)\n",
    "\n",
    "\n",
    "def remove_markers(text: str) -> str:\n",
    "    normalized_tokens = []\n",
    "    for token in text.split():\n",
    "        normalized_tokens.append(re.sub(r'\\u00AE', '', token))\n",
    "    return \" \".join(normalized_tokens)\n",
    "\n",
    "def _normalize_country_names(text: str) -> str:\n",
    "    normalized_tokens = []\n",
    "    for token in text.split():\n",
    "        upper_token = token.upper()\n",
    "        country_name = None\n",
    "\n",
    "        # Try to lookup by alpha-2 code\n",
    "        country = pycountry.countries.get(alpha_2=upper_token)\n",
    "        if not country:\n",
    "            # Try to lookup by alpha-3 code if alpha-2 lookup fails\n",
    "            country = pycountry.countries.get(alpha_3=upper_token)\n",
    "\n",
    "        if country:\n",
    "            country_name = country.name\n",
    "\n",
    "        # Append the found country name or the original token if not found\n",
    "        normalized_tokens.append(country_name if country_name else token)\n",
    "\n",
    "    return \" \".join(normalized_tokens)\n",
    "\n",
    "\n",
    "\n",
    "def convert_numbers(text: str) -> str:\n",
    "    new_text = []\n",
    "    for w in text.split():\n",
    "        try:\n",
    "            w = num2words(int(w))\n",
    "        except:\n",
    "            a = 0\n",
    "        new_text.append(w)\n",
    "    new_text = np.char.replace(new_text, \"-\", \" \")\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "\n",
    "def remove_commas_from_numbers(text: str) -> str:\n",
    "    # Define the regex pattern\n",
    "    pattern = r'(?<=\\d),(?=\\d)'\n",
    "    new_text = []\n",
    "    # Process each string in the list\n",
    "    for w in text.split():\n",
    "        # Replace commas that are between two digits\n",
    "        w = re.sub(pattern, '', w)\n",
    "        new_text.append(w)\n",
    "\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "\n",
    "\n",
    "def lowercase_letters(text: str) -> str:\n",
    "    return text.lower()\n",
    "\n",
    "def lemmatize_words(text: str) -> str:\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tagged_tokens = pos_tag(text.split())\n",
    "    return \" \".join([lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in tagged_tokens])\n",
    "\n",
    "\n",
    "# perform part-of-speech (POS) tagging on the tokens.\n",
    "def get_wordnet_pos(tag: str) -> str:\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def _normalize_dates(text: str) -> str:\n",
    "    date_pattern = r'(\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4})|' \\\n",
    "                   r'(\\d{4}[-/]\\d{1,2}[-/]\\d{1,2})|' \\\n",
    "                   r'(\\d{1,2}\\s+(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s+\\d{2,4})|' \\\n",
    "                   r'((Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s+\\d{1,2},\\s+\\d{2,4})|' \\\n",
    "                   r'(\\d{1,2}\\s+(January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{2,4})|' \\\n",
    "                   r'((January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2},\\s+\\d{2,4})'\n",
    "    format_strings = ['%d-%m-%Y', '%d/%m/%Y', '%d.%m.%Y', '%Y-%m-%d', '%Y/%m/%d', '%Y.%m.%d', '%d %b %Y', '%b %d, %Y',\n",
    "                      '%d %B %Y', '%B %d, %Y', '%m/%d/%Y', '%m-%d-%Y', '%m.%d.%Y', '%d-%m-%y', '%d/%m/%y', '%d.%m.%y',\n",
    "                      '%y-%m-%d', '%y/%m/%d', '%y.%m.%d', '%d %b %y', '%b %d, %y',\n",
    "                      '%d %B %y', '%B %d, %y', '%m/%d/%y', '%m-%d-%y', '%m.%d.%y']\n",
    "    normalized_tokens = []\n",
    "    for token in text.split():\n",
    "        matches = re.findall(date_pattern, token)\n",
    "        if matches:\n",
    "            match = matches[0][0]\n",
    "            for fmt in format_strings:\n",
    "                try:\n",
    "                    date_obj = datetime.strptime(match, fmt)\n",
    "                    break\n",
    "                except ValueError:\n",
    "                    pass\n",
    "            else:\n",
    "                continue\n",
    "            normalized_date = date_obj.strftime('%Y-%m-%d')\n",
    "            token = token.replace(match, normalized_date)\n",
    "        normalized_tokens.append(token)\n",
    "\n",
    "    return \" \".join(normalized_tokens)\n",
    "\n",
    "\n",
    "def remove_apostrophe(text: str) -> str:\n",
    "    new_tokens = []\n",
    "    for token in text.split():\n",
    "        new_tokens.append(str(np.char.replace(token, \"'\", \" \")))\n",
    "    return \" \".join(new_tokens)\n",
    "\n",
    "\n",
    "def normalize_abbreviations(text: str) -> str:\n",
    "    resolved_terms = {}\n",
    "    new_tokens = []\n",
    "\n",
    "    for token in text.split():\n",
    "        synsets = wordnet.synsets(token)\n",
    "        if synsets:\n",
    "            resolved_term = synsets[0].lemmas()[0].name()\n",
    "            resolved_terms[token] = resolved_term\n",
    "            new_tokens.append(resolved_term)\n",
    "        else:\n",
    "            new_tokens.append(token)\n",
    "\n",
    "    return \" \".join(new_tokens)\n",
    "\n",
    "\n",
    "def process_text_quora(text: str) -> str:\n",
    "    lowercase = lowercase_letters(text)\n",
    "    # normalize_numbers = remove_commas_from_numbers(lowercase)\n",
    "    # num2word = convert_numbers(normalize_numbers)\n",
    "    punctuations_removed = remove_punctuations(lowercase)\n",
    "    apostrophe_removed = remove_apostrophe(punctuations_removed)\n",
    "    stopwords_removed = remove_stopwords(apostrophe_removed, 'quora')\n",
    "    # markers_removed = remove_markers(stopwords_removed)\n",
    "    # stemmed = stem_words(markers_removed)\n",
    "    normalized_dates = _normalize_dates(stopwords_removed)\n",
    "    normalized_country_names = _normalize_country_names(normalized_dates)\n",
    "    abbreviations = normalize_abbreviations(normalized_country_names)\n",
    "    lowercase = lowercase_letters(abbreviations)\n",
    "    new_tokens = replace_under_score_with_space(lowercase)\n",
    "    lemmatized = lemmatize_words(new_tokens)\n",
    "    new_tokens = lemmatized\n",
    "    return new_tokens\n",
    "\n",
    "\n",
    "def process_text_clinical(text: str) -> str:\n",
    "    lowercase = lowercase_letters(text)\n",
    "    normalize_numbers = remove_commas_from_numbers(lowercase)\n",
    "    num2word = convert_numbers(normalize_numbers)\n",
    "    punctuations_removed = remove_punctuations(num2word)\n",
    "    apostrophe_removed = remove_apostrophe(punctuations_removed)\n",
    "    stopwords_removed = remove_stopwords(apostrophe_removed, 'clinical')\n",
    "    markers_removed = remove_markers(stopwords_removed)\n",
    "    # stemmed = stem_words(markers_removed)\n",
    "    normalized_dates = _normalize_dates(markers_removed)\n",
    "    normalized_country_names = _normalize_country_names(normalized_dates)\n",
    "    abbreviations = normalize_abbreviations(normalized_country_names)\n",
    "    lowercase = lowercase_letters(abbreviations)\n",
    "    new_tokens = replace_under_score_with_space(lowercase)\n",
    "    lemmatized = lemmatize_words(new_tokens)\n",
    "    new_tokens = lemmatized\n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e4f7194c-67ea-46af-be0f-928d1a7dc58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import ir_datasets\n",
    "from nltk import word_tokenize\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "quora_dataset = ir_datasets.load(\"beir/quora/test\")\n",
    "clinical_dataset = ir_datasets.load(\"clinicaltrials/2017/trec-pm-2017\")\n",
    "\n",
    "def save_model(obj, file_name: str):\n",
    "    with open(file_name, 'wb') as file:\n",
    "        pickle.dump(obj, file)\n",
    "\n",
    "\n",
    "def load_model(file_name):\n",
    "    with open(file_name, 'rb') as file:\n",
    "        model = pickle.load(file)\n",
    "    return model\n",
    "\n",
    "\n",
    "class QuoraMyCorpus:\n",
    "    documents = []\n",
    "    documents_ids = []\n",
    "    i = 0\n",
    "    def __iter__(self):\n",
    "        for doc in quora_dataset.docs_iter():\n",
    "            if QuoraMyCorpus.i >= 522931:\n",
    "                break\n",
    "            tokens = process_text_quora(doc[1])\n",
    "            QuoraMyCorpus.i += 1\n",
    "            QuoraMyCorpus.documents.append(tokens)\n",
    "            QuoraMyCorpus.documents_ids.append(doc[0])\n",
    "            yield tokens\n",
    "\n",
    "\n",
    "def get_embedding_vector(model, tokens):\n",
    "    embeddings = []\n",
    "    size = model.vector_size\n",
    "    if len(tokens) < 1:\n",
    "        return np.zeros(size)\n",
    "    else:\n",
    "        for token in tokens:\n",
    "            if token in model.wv.index_to_key:\n",
    "                embeddings.append(model.wv.get_vector(token))\n",
    "            else:\n",
    "                embeddings.append(np.zeros(size))\n",
    "    return np.mean(embeddings, axis=0)\n",
    "\n",
    "\n",
    "# save word 2 vec model for quora\n",
    "def word_2_vector_for_quora():\n",
    "    sentences = QuoraMyCorpus()\n",
    "    z = [line.split() for line in sentences]\n",
    "    print(\"before word 2 vec model\")\n",
    "    model = gensim.models.Word2Vec(sentences=z, min_count=1, workers=4)\n",
    "    save_model(model, 'quora_word_2_vector.pickle')\n",
    "    print(\"after word 2 vec model\")\n",
    "    documents = QuoraMyCorpus.documents\n",
    "    print(\"before train_matrix\")\n",
    "    train_matrix = [get_embedding_vector(model, word_tokenize(d)) for d in documents]\n",
    "    save_model(train_matrix, 'matrix_word_2_vector.pickle')\n",
    "    print(\"after train_matrix\")\n",
    "\n",
    "\n",
    "\n",
    "class ClinicalMyCorpus:\n",
    "    documents = []\n",
    "    documents_ids = []\n",
    "\n",
    "    def __iter__(self):\n",
    "        for doc in clinical_dataset.docs_iter():\n",
    "            tokens = process_text_clinical(doc[1] + ' ' + doc[2] + ' ' + doc[3] + ' ' + doc[4] + ' ' + doc[5])\n",
    "            ClinicalMyCorpus.documents.append(tokens)\n",
    "            ClinicalMyCorpus.documents_ids.append(doc[0])\n",
    "            yield tokens\n",
    "\n",
    "\n",
    "# save word 2 vec model for clinical\n",
    "def word_2_vector_for_clinical():\n",
    "    sentences = ClinicalMyCorpus()\n",
    "    z = [line.split() for line in sentences]\n",
    "    print(\"before word 2 vec model\")\n",
    "    model = gensim.models.Word2Vec(sentences=z, min_count=1, workers=4)\n",
    "    save_model(model, 'clinical_word_2_vector.pickle')\n",
    "    print(\"after word 2 vec model\")\n",
    "    documents = ClinicalMyCorpus.documents\n",
    "    print(\"before train_matrix\")\n",
    "    train_matrix = [get_embedding_vector(model, word_tokenize(d)) for d in documents]\n",
    "    save_model(train_matrix, 'clinical_matrix_word_2_vector.pickle')\n",
    "    print(\"after train_matrix\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "03c707b6-c956-4359-a317-ebf36d3e0a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ir_measures\n",
    "from ir_measures import AP, P, R, RR\n",
    "from nltk import word_tokenize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def match_query(dataset, query: str, matrix, model, docs_id) -> dict:\n",
    "    if dataset == QuoraConstant.name:\n",
    "        query = process_text_quora(query)\n",
    "        similarity_ratio = QuoraConstant.cosine_threshold\n",
    "    else:\n",
    "        query = process_text_clinical(query)\n",
    "        similarity_ratio = ClinicalConstant.cosine_threshold\n",
    "    query_vector = get_embedding_vector(model, word_tokenize(query))\n",
    "    matched_documents = {}\n",
    "    similarity = cosine_similarity(matrix, [query_vector])\n",
    "    for i, s in enumerate(similarity):\n",
    "        if s >= similarity_ratio:\n",
    "            matched_documents[docs_id[i]] = float(s[0])\n",
    "    return matched_documents\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(dataset_collection, dataset_name):\n",
    "    qrels = dataset_collection.qrels_iter()\n",
    "    queries = dataset_collection.queries_iter()\n",
    "    if dataset_name == QuoraConstant.name:\n",
    "        model = load_model('quora_word_2_vector.pickle')\n",
    "        matrix = load_model('matrix_word_2_vector.pickle')\n",
    "    else:\n",
    "        model = load_model('clinical_word_2_vector.pickle')\n",
    "        matrix = load_model('clinical_matrix_word_2_vector.pickle')\n",
    "    documents_ids = []\n",
    "    for d in dataset_collection.docs_iter():\n",
    "        documents_ids.append(d[0])\n",
    "    print('start evaluation')\n",
    "    i = 0\n",
    "    ranking_docs = dict()\n",
    "    for query in queries:\n",
    "        i += 1\n",
    "        print(i)\n",
    "        if dataset_name == QuoraConstant.name:\n",
    "            retrieved_docs = match_query(dataset_name, query.text, matrix, model, documents_ids)\n",
    "        else:\n",
    "            if query.other == 'None':\n",
    "                text = query.disease + ' ' + query.gene + ' ' + query.demographic\n",
    "            else:\n",
    "                text = query.disease + ' ' + query.gene + ' ' + query.demographic + ' ' + query.other\n",
    "            retrieved_docs = match_query(dataset_name, text, matrix, model, documents_ids)\n",
    "        ranking_docs[query.query_id] = retrieved_docs\n",
    "    metrics = [AP(rel=1), P(rel=1) @ 10, R(rel=1) @ 10, RR(rel=1)]\n",
    "    qrels_map = dict()\n",
    "    for qrel in qrels:\n",
    "        if qrel.query_id in ranking_docs.keys():\n",
    "            if qrel.query_id in qrels_map:\n",
    "                qrels_map[qrel.query_id].update({qrel.doc_id: qrel.relevance})\n",
    "            else:\n",
    "                qrels_map[qrel.query_id] = {qrel.doc_id: qrel.relevance}\n",
    "    score = ir_measures.calc_aggregate(metrics, qrels_map, ranking_docs)\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "36b72244-842c-4d28-99b7-091f041a367b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start evaluation\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "Evaluation score with word embedding for clinical: {AP: 0.0026224844654136143, P@10: 0.006896551724137932, R@10: 0.0010756495949714488, RR: 0.03169902802997383}\n"
     ]
    }
   ],
   "source": [
    "c_score = evaluate(clinical_dataset, 'clinical')\n",
    "print(\"Evaluation score with word embedding for clinical:\", c_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecee9a18-09a8-4a7b-9587-5725cd0536ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start evaluation\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "score = evaluate(quora_dataset, 'quora')\n",
    "print(\"Evaluation score with word embedding for beir/quora/test:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253eb113-68a6-4ae2-ab51-3d335a02c41e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
