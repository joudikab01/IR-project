{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27ba5273-8dee-4466-879f-a1ca0fe1aaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuoraConstant:\n",
    "    link = 'beir/quora/test'\n",
    "    name = 'quora'\n",
    "    cosine_threshold = 0.3\n",
    "    ngram = (1, 2)\n",
    "    min_df = 2\n",
    "    max_df = 0.8\n",
    "    vectorized_path = 'quora_vectorizer.pkl'\n",
    "    cluster_path = 'quora_cluster.pkl'\n",
    "\n",
    "class ClinicalConstant:\n",
    "    link = 'clinicaltrials/2017/trec-pm-2017'\n",
    "    name = 'clinical'\n",
    "    cosine_threshold = 0.001\n",
    "    ngram = (1, 3)\n",
    "    min_df = 2\n",
    "    max_df = 0.8\n",
    "    vectorized_path = 'clinical_vectorizer.pkl'\n",
    "    cluster_path = 'clinical_cluster.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96790edd-73d5-4188-aa48-18d5f4936873",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import re\n",
    "from nltk import WordNetLemmatizer, pos_tag\n",
    "import num2words\n",
    "import numpy as np\n",
    "import pycountry\n",
    "from nltk import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def replace_under_score_with_space(text: str) -> str:\n",
    "    new_tokens = []\n",
    "    for token in text.split():\n",
    "        new_tokens.append(re.sub(r'_', ' ', token))\n",
    "    return \" \".join(new_tokens)\n",
    "\n",
    "\n",
    "def remove_stopwords(text: str, dataset: str) -> str:\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    questions = {'what', 'which', 'who', 'where', 'why', 'when', 'how', 'whose', 'how often', 'how long', 'how far',\n",
    "                 'how old', 'how come', 'how much', 'how many', 'what type', 'what kind', 'which type', 'which kind'}\n",
    "    if dataset == 'quora':\n",
    "        stop_words = stop_words - questions\n",
    "    else:\n",
    "        stop_words = stop_words - {'a'}\n",
    "    return \" \".join([word for word in str(text).split() if word not in stop_words])\n",
    "\n",
    "def remove_punctuations(text: str) -> str:\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    result = tokenizer.tokenize(text)\n",
    "    return \" \".join(result)\n",
    "\n",
    "\n",
    "def remove_markers(text: str) -> str:\n",
    "    normalized_tokens = []\n",
    "    for token in text.split():\n",
    "        normalized_tokens.append(re.sub(r'\\u00AE', '', token))\n",
    "    return \" \".join(normalized_tokens)\n",
    "\n",
    "def _normalize_country_names(text: str) -> str:\n",
    "    normalized_tokens = []\n",
    "    for token in text.split():\n",
    "        upper_token = token.upper()\n",
    "        country_name = None\n",
    "\n",
    "        # Try to lookup by alpha-2 code\n",
    "        country = pycountry.countries.get(alpha_2=upper_token)\n",
    "        if not country:\n",
    "            # Try to lookup by alpha-3 code if alpha-2 lookup fails\n",
    "            country = pycountry.countries.get(alpha_3=upper_token)\n",
    "\n",
    "        if country:\n",
    "            country_name = country.name\n",
    "\n",
    "        # Append the found country name or the original token if not found\n",
    "        normalized_tokens.append(country_name if country_name else token)\n",
    "\n",
    "    return \" \".join(normalized_tokens)\n",
    "\n",
    "\n",
    "\n",
    "def convert_numbers(text: str) -> str:\n",
    "    new_text = []\n",
    "    for w in text.split():\n",
    "        try:\n",
    "            w = num2words(int(w))\n",
    "        except:\n",
    "            a = 0\n",
    "        new_text.append(w)\n",
    "    new_text = np.char.replace(new_text, \"-\", \" \")\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "\n",
    "def remove_commas_from_numbers(text: str) -> str:\n",
    "    # Define the regex pattern\n",
    "    pattern = r'(?<=\\d),(?=\\d)'\n",
    "    new_text = []\n",
    "    # Process each string in the list\n",
    "    for w in text.split():\n",
    "        # Replace commas that are between two digits\n",
    "        w = re.sub(pattern, '', w)\n",
    "        new_text.append(w)\n",
    "\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "\n",
    "\n",
    "def lowercase_letters(text: str) -> str:\n",
    "    return text.lower()\n",
    "\n",
    "def lemmatize_words(text: str) -> str:\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tagged_tokens = pos_tag(text.split())\n",
    "    return \" \".join([lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in tagged_tokens])\n",
    "\n",
    "\n",
    "# perform part-of-speech (POS) tagging on the tokens.\n",
    "def get_wordnet_pos(tag: str) -> str:\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def _normalize_dates(text: str) -> str:\n",
    "    date_pattern = r'(\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4})|' \\\n",
    "                   r'(\\d{4}[-/]\\d{1,2}[-/]\\d{1,2})|' \\\n",
    "                   r'(\\d{1,2}\\s+(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s+\\d{2,4})|' \\\n",
    "                   r'((Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s+\\d{1,2},\\s+\\d{2,4})|' \\\n",
    "                   r'(\\d{1,2}\\s+(January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{2,4})|' \\\n",
    "                   r'((January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2},\\s+\\d{2,4})'\n",
    "    format_strings = ['%d-%m-%Y', '%d/%m/%Y', '%d.%m.%Y', '%Y-%m-%d', '%Y/%m/%d', '%Y.%m.%d', '%d %b %Y', '%b %d, %Y',\n",
    "                      '%d %B %Y', '%B %d, %Y', '%m/%d/%Y', '%m-%d-%Y', '%m.%d.%Y', '%d-%m-%y', '%d/%m/%y', '%d.%m.%y',\n",
    "                      '%y-%m-%d', '%y/%m/%d', '%y.%m.%d', '%d %b %y', '%b %d, %y',\n",
    "                      '%d %B %y', '%B %d, %y', '%m/%d/%y', '%m-%d-%y', '%m.%d.%y']\n",
    "    normalized_tokens = []\n",
    "    for token in text.split():\n",
    "        matches = re.findall(date_pattern, token)\n",
    "        if matches:\n",
    "            match = matches[0][0]\n",
    "            for fmt in format_strings:\n",
    "                try:\n",
    "                    date_obj = datetime.strptime(match, fmt)\n",
    "                    break\n",
    "                except ValueError:\n",
    "                    pass\n",
    "            else:\n",
    "                continue\n",
    "            normalized_date = date_obj.strftime('%Y-%m-%d')\n",
    "            token = token.replace(match, normalized_date)\n",
    "        normalized_tokens.append(token)\n",
    "\n",
    "    return \" \".join(normalized_tokens)\n",
    "\n",
    "\n",
    "def remove_apostrophe(text: str) -> str:\n",
    "    new_tokens = []\n",
    "    for token in text.split():\n",
    "        new_tokens.append(str(np.char.replace(token, \"'\", \" \")))\n",
    "    return \" \".join(new_tokens)\n",
    "\n",
    "\n",
    "def normalize_abbreviations(text: str) -> str:\n",
    "    resolved_terms = {}\n",
    "    new_tokens = []\n",
    "\n",
    "    for token in text.split():\n",
    "        synsets = wordnet.synsets(token)\n",
    "        if synsets:\n",
    "            resolved_term = synsets[0].lemmas()[0].name()\n",
    "            resolved_terms[token] = resolved_term\n",
    "            new_tokens.append(resolved_term)\n",
    "        else:\n",
    "            new_tokens.append(token)\n",
    "\n",
    "    return \" \".join(new_tokens)\n",
    "\n",
    "\n",
    "def process_text_quora(text: str) -> str:\n",
    "    lowercase = lowercase_letters(text)\n",
    "    # normalize_numbers = remove_commas_from_numbers(lowercase)\n",
    "    # num2word = convert_numbers(normalize_numbers)\n",
    "    punctuations_removed = remove_punctuations(lowercase)\n",
    "    apostrophe_removed = remove_apostrophe(punctuations_removed)\n",
    "    stopwords_removed = remove_stopwords(apostrophe_removed, 'quora')\n",
    "    # markers_removed = remove_markers(stopwords_removed)\n",
    "    # stemmed = stem_words(markers_removed)\n",
    "    normalized_dates = _normalize_dates(stopwords_removed)\n",
    "    normalized_country_names = _normalize_country_names(normalized_dates)\n",
    "    abbreviations = normalize_abbreviations(normalized_country_names)\n",
    "    lowercase = lowercase_letters(abbreviations)\n",
    "    new_tokens = replace_under_score_with_space(lowercase)\n",
    "    lemmatized = lemmatize_words(new_tokens)\n",
    "    new_tokens = lemmatized\n",
    "    return new_tokens\n",
    "\n",
    "\n",
    "def process_text_clinical(text: str) -> str:\n",
    "    lowercase = lowercase_letters(text)\n",
    "    normalize_numbers = remove_commas_from_numbers(lowercase)\n",
    "    num2word = convert_numbers(normalize_numbers)\n",
    "    punctuations_removed = remove_punctuations(num2word)\n",
    "    apostrophe_removed = remove_apostrophe(punctuations_removed)\n",
    "    stopwords_removed = remove_stopwords(apostrophe_removed, 'clinical')\n",
    "    markers_removed = remove_markers(stopwords_removed)\n",
    "    # stemmed = stem_words(markers_removed)\n",
    "    normalized_dates = _normalize_dates(markers_removed)\n",
    "    normalized_country_names = _normalize_country_names(normalized_dates)\n",
    "    abbreviations = normalize_abbreviations(normalized_country_names)\n",
    "    lowercase = lowercase_letters(abbreviations)\n",
    "    new_tokens = replace_under_score_with_space(lowercase)\n",
    "    lemmatized = lemmatize_words(new_tokens)\n",
    "    new_tokens = lemmatized\n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531d9032",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "def save_cluster_labels(dataset_name: str, cluster_labels: dict):\n",
    "    # Move up to the project root directory\n",
    "    project_root_dir = os.path.join(\n",
    "        os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))),\n",
    "        'clustering')\n",
    "    with open(project_root_dir + '/' + dataset_name + '_cluster_labels.pickle', 'wb') as file:\n",
    "        pickle.dump(cluster_labels, file)\n",
    "\n",
    "\n",
    "def load_cluster_labels(dataset_name: str) -> dict:\n",
    "    project_root_dir = os.path.join(\n",
    "        os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))),\n",
    "        'clustering')\n",
    "    with open(project_root_dir + '/' + dataset_name + '_cluster_labels.pickle', 'rb') as file:\n",
    "        cluster_labels = pickle.load(file)\n",
    "    return cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2c0e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "\n",
    "def save_tfidf_matrix(dataset_name: str, inverted_index: dict):\n",
    "    # Move up to the project root directory\n",
    "    project_root_dir = os.path.join(\n",
    "        os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))),\n",
    "        'db')\n",
    "    with open(project_root_dir + '/' + dataset_name + '_inverted_index.pickle', 'wb') as file:\n",
    "        pickle.dump(inverted_index, file)\n",
    "\n",
    "\n",
    "def load_tfidf_matrix(dataset_name: str) -> dict:\n",
    "    project_root_dir = os.path.join(\n",
    "        os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))),\n",
    "        'db')\n",
    "    with open(project_root_dir + '/' + dataset_name + '_inverted_index.pickle', 'rb') as file:\n",
    "        tfidf_matrix = pickle.load(file)\n",
    "    return tfidf_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d63f1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "import ir_datasets\n",
    "\n",
    "from db import DataBase\n",
    "\n",
    "\n",
    "def get_dataset_docs(dataset_name: str) -> Dict[str, str]:\n",
    "    print(\"get dataset docs \" + dataset_name)\n",
    "    if dataset_name == \"clinical\":\n",
    "        dataset = ir_datasets.load(\"clinicaltrials/2017/trec-pm-2017\")\n",
    "        docs_iter = dataset.docs_iter()\n",
    "        random_corpus = {}\n",
    "        for doc in docs_iter:\n",
    "            doc_id = doc[0]\n",
    "            detailed_description = (doc[1] + ' ' + doc[2] + ' ' + doc[3] + ' '\n",
    "                                    + doc[4] + ' ' + doc[5])\n",
    "            random_corpus[doc_id] = detailed_description\n",
    "        random_corpus_ids = set(random_corpus.keys())\n",
    "        search_qrels = list(ir_datasets.load(\"clinicaltrials/2017/trec-pm-2017\").qrels_iter())\n",
    "        qrels_docs_ids = set(qrel.doc_id for qrel in search_qrels)\n",
    "        docs_ids = random_corpus_ids.union(qrels_docs_ids)\n",
    "        docs_store = ir_datasets.load(\"clinicaltrials/2017/trec-pm-2017\").docs_store()\n",
    "        mapped_docs = dict(docs_store.get_many(docs_ids))\n",
    "        corpus = {\n",
    "            doc_id: doc.title + ' ' + doc.condition + ' ' + doc.summary +\n",
    "                    ' ' + doc.detailed_description + ' ' + doc.eligibility\n",
    "            for doc_id, doc in mapped_docs.items()\n",
    "        }\n",
    "    else:  # dataset_name == \"quora\":\n",
    "        random_corpus = dict(ir_datasets.load(\"beir/quora/test\").docs_iter())\n",
    "        random_corpus_ids = set(random_corpus.keys())\n",
    "        qrels = list(ir_datasets.load(\"beir/quora/test\").qrels_iter())\n",
    "        qrels_docs_ids = set(qrel.doc_id for qrel in qrels)\n",
    "        docs_ids = random_corpus_ids.union(qrels_docs_ids)\n",
    "        docs_store = ir_datasets.load(\"beir/quora/test\").docs_store()\n",
    "        mapped_docs = dict(docs_store.get_many(docs_ids))\n",
    "        corpus = {}\n",
    "        for doc_id, doc in mapped_docs.items():\n",
    "            doc_id = doc[0]\n",
    "            detailed_description = (doc[1])\n",
    "            corpus[doc_id] = detailed_description\n",
    "\n",
    "    print(len(corpus))\n",
    "    db = DataBase.db\n",
    "    collection = db[dataset_name]\n",
    "    collection.drop()\n",
    "    documents_to_insert = []\n",
    "    index = 0\n",
    "    for doc_id, text in corpus.items():\n",
    "        documents_to_insert.append({'index': index, \"_id\": doc_id, \"text\": text})\n",
    "        index += 1\n",
    "    collection.insert_many(documents_to_insert)\n",
    "    return corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e37583d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inverted index with TF-IDF weights\n",
    "import os\n",
    "\n",
    "import joblib\n",
    "from nltk import word_tokenize\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "def create_inverted_index(dataset: str):\n",
    "    print(\"create inverted index \" + dataset)\n",
    "    corpus = get_dataset_docs(dataset)\n",
    "    print(\"after get_dataset_docs \" + dataset)\n",
    "    if dataset == QuoraConstant.name:\n",
    "        vectorizer = TfidfVectorizer(preprocessor=process_text_quora, tokenizer=word_tokenize,\n",
    "                                     # max_df=QuoraConstant.max_df,\n",
    "                                     # min_df=QuoraConstant.min_df,\n",
    "                                     # norm=\"l2\",\n",
    "                                     # ngram_range=QuoraConstant.ngram,\n",
    "                                     # sublinear_tf=True,\n",
    "                                     # use_idf=True,\n",
    "                                     # use_idf=True,\n",
    "\n",
    "                                     )\n",
    "\n",
    "    else:\n",
    "        vectorizer = TfidfVectorizer(preprocessor=process_text_clinical, tokenizer=word_tokenize,\n",
    "                                     # max_df=ClinicalConstant.max_df,\n",
    "                                     # min_df=ClinicalConstant.min_df,\n",
    "                                     norm=\"l2\",\n",
    "                                     # ngram_range=ClinicalConstant.ngram,\n",
    "                                     sublinear_tf=True,\n",
    "                                     use_idf=True,\n",
    "                                     stop_words='english',\n",
    "                                     lowercase=False,\n",
    "                                     )\n",
    "\n",
    "    print(\"before fit transform \" + dataset)\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus.values())\n",
    "    project_root_dir = os.path.join(\n",
    "        os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))),\n",
    "        'vectorizer')\n",
    "    if dataset == QuoraConstant.name:\n",
    "        joblib.dump(vectorizer, project_root_dir + '/' + QuoraConstant.vectorized_path)\n",
    "    else:\n",
    "        joblib.dump(vectorizer, project_root_dir + '/' + ClinicalConstant.vectorized_path)\n",
    "    print(\"after fit transform \" + dataset)\n",
    "    save_tfidf_matrix(dataset, tfidf_matrix)\n",
    "    # save_doc_ids(dataset, corpus.keys())\n",
    "    return tfidf_matrix\n",
    "\n",
    "\n",
    "def create_inverted_index_with_clustering(dataset: str):\n",
    "    print(\"create inverted index with clustering \" + dataset)\n",
    "    corpus = get_dataset_docs(dataset)\n",
    "    print(\"after get_dataset_docs with clustering\" + dataset)\n",
    "    if dataset == QuoraConstant.name:\n",
    "        vectorizer = TfidfVectorizer(preprocessor=process_text_quora, tokenizer=word_tokenize,)\n",
    "\n",
    "    else:\n",
    "        vectorizer = TfidfVectorizer(preprocessor=process_text_clinical, tokenizer=word_tokenize,\n",
    "                                     norm=\"l2\",\n",
    "                                     sublinear_tf=True,\n",
    "                                     use_idf=True,\n",
    "                                     stop_words='english',\n",
    "                                     lowercase=False,\n",
    "                                     )\n",
    "\n",
    "    print(\"before fit transform \" + dataset)\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus.values())\n",
    "    project_root_dir = os.path.join(\n",
    "        os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))),\n",
    "        'vectorizer')\n",
    "    if dataset == QuoraConstant.name:\n",
    "        joblib.dump(vectorizer, project_root_dir + '/' + QuoraConstant.vectorized_path)\n",
    "    else:\n",
    "        joblib.dump(vectorizer, project_root_dir + '/' + ClinicalConstant.vectorized_path)\n",
    "    print(\"after fit transform \" + dataset)\n",
    "    save_tfidf_matrix(dataset, tfidf_matrix)\n",
    "    kmeans = KMeans(n_clusters=3, random_state=2)\n",
    "    cluster_labels = kmeans.fit_predict(tfidf_matrix)\n",
    "    project_root_dir = os.path.join(\n",
    "        os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))),\n",
    "        'clustering')\n",
    "    if dataset == QuoraConstant.name:\n",
    "        joblib.dump(kmeans, project_root_dir + '/' + QuoraConstant.cluster_path)\n",
    "    else:\n",
    "        joblib.dump(kmeans, project_root_dir + '/' + ClinicalConstant.cluster_path)\n",
    "    print(\"after kmeans fit \" + dataset)\n",
    "    save_cluster_labels(dataset, cluster_labels)\n",
    "    return tfidf_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d12dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import joblib\n",
    "\n",
    "\n",
    "class VectorizeQuery:\n",
    "    project_root_dir = os.path.join(\n",
    "        os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))),\n",
    "        'vectorizer')\n",
    "    quora_vectorizer = joblib.load( project_root_dir + '\\\\' + QuoraConstant.vectorized_path)\n",
    "    clinica_vectorizer = joblib.load(project_root_dir + '\\\\' + ClinicalConstant.vectorized_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_vectorize(query, dataset):\n",
    "        if dataset == QuoraConstant.name:\n",
    "            query_vector = VectorizeQuery.quora_vectorizer\n",
    "        else:\n",
    "            query_vector = VectorizeQuery.clinica_vectorizer\n",
    "        return query_vector.transform([query])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e10fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def ranking_with_clustering(query: str, dataset: str, tfidf_matrix, kmeans_model, clustered_labels) -> Dict[str, float]:\n",
    "    if dataset == QuoraConstant.name:\n",
    "        cosine_threshold = QuoraConstant.cosine_threshold\n",
    "    else:\n",
    "        cosine_threshold = ClinicalConstant.cosine_threshold\n",
    "\n",
    "    query_vector = VectorizeQuery.get_vectorize(query, dataset)\n",
    "    target_cluster = kmeans_model.predict(query_vector.reshape(1, -1))[0]\n",
    "\n",
    "    cluster_indices = np.where(np.array(clustered_labels) == target_cluster)[0]\n",
    "    cluster_matrix = tfidf_matrix[cluster_indices]\n",
    "\n",
    "    similarities = cosine_similarity(query_vector.reshape(1, -1), cluster_matrix)\n",
    "\n",
    "    # First, filter documents using a similarity threshold\n",
    "    threshold_indices = np.where(similarities >= cosine_threshold)[1]\n",
    "\n",
    "    # Then, rank the filtered documents based on their similarity scores\n",
    "    ranked_indices = np.argsort(similarities[0][threshold_indices])[-10:][::-1] \n",
    "\n",
    "    results = {}\n",
    "    for index in ranked_indices:\n",
    "        doc_id = str(cluster_indices[threshold_indices[index]])\n",
    "        similarity = similarities[0][threshold_indices[index]]\n",
    "        results[doc_id] = similarity\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f444e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "def customize_result_from_index_to_doc_id(result, dataset: str):\n",
    "    mongo_client = MongoClient('localhost', 27017)\n",
    "    db = mongo_client['IR_DOCS']\n",
    "    collection = db[dataset]\n",
    "    data = collection.find({\"index\": {\"$in\": [int(s) for s in list(result)]}}, {'_id': 1, 'index': 1})\n",
    "    new_result = {}\n",
    "    for item in data:\n",
    "        new_result[item['_id']] = result[str(item['index'])]\n",
    "    return new_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b396878",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_with_clustering(query, dataset, tfidf_matrix, kmeans_model, cluster_labels) -> Dict[str, float]:\n",
    "    result = ranking_with_clustering(query, dataset, tfidf_matrix, kmeans_model, cluster_labels)\n",
    "    return customize_result_from_index_to_doc_id(result, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20337810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ir_measures\n",
    "from ir_measures import AP, P, R, RR\n",
    "\n",
    "def evaluate_with_clustering(dataset_collection, dataset_name):\n",
    "    qrels = dataset_collection.qrels_iter()\n",
    "    queries = dataset_collection.queries_iter()\n",
    "    ranking_docs = dict()\n",
    "    i = 0\n",
    "    tfidf_matrix = load_tfidf_matrix(dataset_name)\n",
    "    clustering_labels = load_cluster_labels(dataset_name)\n",
    "    project_root_dir = os.path.join(\n",
    "        os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))),\n",
    "        'clustering')\n",
    "    if dataset_name == QuoraConstant.name:\n",
    "        cluster_model = joblib.load(project_root_dir + '\\\\' + QuoraConstant.cluster_path)\n",
    "    else:\n",
    "        cluster_model = joblib.load(project_root_dir + '\\\\' + ClinicalConstant.cluster_path)\n",
    "    for query in queries:\n",
    "        if dataset_name == QuoraConstant.name:\n",
    "            retrieved_docs = retrieve_with_clustering(query.text, dataset_name, tfidf_matrix, cluster_model,\n",
    "                                                      clustering_labels)\n",
    "        else:\n",
    "            if query.other == 'None':\n",
    "                text = query.disease + ' ' + query.gene + ' ' + query.demographic\n",
    "            else:\n",
    "                text = query.disease + ' ' + query.gene + ' ' + query.demographic + ' ' + query.other\n",
    "            print(text)\n",
    "            retrieved_docs = retrieve_with_clustering(text, dataset_name, tfidf_matrix, cluster_model,\n",
    "                                                      clustering_labels)\n",
    "        ranking_docs[query.query_id] = retrieved_docs\n",
    "        i += 1\n",
    "        if i % 100 == 0:\n",
    "            print(i)\n",
    "    metrics = [AP(rel=1), P(rel=1) @ 10, R(rel=1) @ 10, RR(rel=1)]\n",
    "    qrels_map = dict()\n",
    "    for qrel in qrels:\n",
    "        if qrel.query_id in ranking_docs.keys():\n",
    "            if qrel.query_id in qrels_map:\n",
    "                qrels_map[qrel.query_id].update({qrel.doc_id: qrel.relevance})\n",
    "            else:\n",
    "                qrels_map[qrel.query_id] = {qrel.doc_id: qrel.relevance}\n",
    "    score = ir_measures.calc_aggregate(metrics, qrels_map, ranking_docs)\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "36b72244-842c-4d28-99b7-091f041a367b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start evaluation\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "Evaluation score with word embedding for clinical: {AP: 0.0026224844654136143, P@10: 0.006896551724137932, R@10: 0.0010756495949714488, RR: 0.03169902802997383}\n"
     ]
    }
   ],
   "source": [
    "quora_dataset = ir_datasets.load(\"beir/quora/test\")\n",
    "clinical_dataset = ir_datasets.load(\"clinicaltrials/2017/trec-pm-2017\")\n",
    "c_score = evaluate_with_clustering(clinical_dataset, 'clinical')\n",
    "print(\"Evaluation score with word embedding for clinical:\", c_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecee9a18-09a8-4a7b-9587-5725cd0536ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start evaluation\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "score = evaluate_with_clustering(quora_dataset, 'quora')\n",
    "print(\"Evaluation score with word embedding for beir/quora/test:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253eb113-68a6-4ae2-ab51-3d335a02c41e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
